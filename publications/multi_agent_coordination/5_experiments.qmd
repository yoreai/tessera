# Experiments and Results

This section presents experimental evaluation of the Multi-Agent Geospatial Coordination Protocol on California fire hazard data.

## Experimental Setup

### Dataset

| Component | Size | Description |
|-----------|------|-------------|
| Addresses | 546,247 | California residential/commercial addresses |
| Fire Hazard Zones | 1,955 | CAL FIRE Very High/Moderate zones |
| Ground Truth Labels | 546,247 | Historical fire intersection + expert review |
| Test Period | 2017-2023 | Validation against actual fire events |

: Dataset summary. {#tbl-dataset}

### Baselines

We compare MACP against:

1. **Single Model (XGBoost)**: Gradient boosting on all features, no agent decomposition
2. **Single Model (Neural Network)**: MLP with same total parameters as MACP
3. **Voting Ensemble**: Unweighted majority vote of 128 models
4. **Bagging Ensemble**: Bootstrap aggregated models without specialization
5. **Mixture of Experts**: Gated mixture without consensus protocol

### Metrics

- **Accuracy**: Overall correct classification rate
- **Precision/Recall/F1**: Class-weighted metrics
- **AUC-ROC**: Area under ROC curve
- **Agreement**: Intra-pool score variance
- **Throughput**: Addresses processed per second
- **Fault Tolerance**: Accuracy under simulated failures

## Classification Results

### Overall Performance

| Method | Accuracy | Precision | Recall | F1 | AUC |
|--------|----------|-----------|--------|-----|-----|
| XGBoost (single) | 0.812 | 0.789 | 0.803 | 0.796 | 0.867 |
| Neural Net (single) | 0.798 | 0.774 | 0.791 | 0.782 | 0.851 |
| Voting Ensemble | 0.845 | 0.823 | 0.838 | 0.830 | 0.901 |
| Bagging Ensemble | 0.856 | 0.834 | 0.847 | 0.840 | 0.912 |
| Mixture of Experts | 0.867 | 0.845 | 0.859 | 0.852 | 0.923 |
| **MACP (ours)** | **0.897** | **0.878** | **0.889** | **0.883** | **0.943** |

: Classification performance comparison. {#tbl-classification}

MACP achieves **89.7% accuracy**, outperforming:

- Single XGBoost by **8.5 percentage points**
- Mixture of Experts by **3.0 percentage points**

### Performance by Pool

| Pool | Accuracy (Alone) | Contribution to MACP |
|------|-----------------|---------------------|
| Wildfire | 0.823 | +0.045 |
| Flood | 0.712 | +0.012 |
| Seismic | 0.698 | +0.008 |
| Analytics | 0.856 | +0.032 |

: Individual pool performance and marginal contribution. {#tbl-pool-performance}

The Wildfire Pool alone achieves 82.3% accuracy; adding other pools and the consensus protocol improves accuracy to 89.7%.

### By Risk Level

| Risk Level | Precision | Recall | F1 | Support |
|------------|-----------|--------|-----|---------|
| Very High | 0.912 | 0.934 | 0.923 | 89,247 |
| High | 0.891 | 0.872 | 0.881 | 143,892 |
| Moderate | 0.856 | 0.867 | 0.861 | 178,456 |
| Low | 0.879 | 0.894 | 0.886 | 134,652 |

: Per-class performance. {#tbl-per-class}

MACP achieves **93.4% recall** on Very High risk addresses---critical for emergency response where missing high-risk locations is costly.

## Consensus Analysis

### Agreement Statistics

| Pool | Mean Agreement | Std Agreement | Mean Confidence |
|------|---------------|---------------|-----------------|
| Wildfire | 0.923 | 0.034 | 0.871 |
| Flood | 0.912 | 0.041 | 0.834 |
| Seismic | 0.897 | 0.048 | 0.812 |
| Analytics | 0.934 | 0.028 | 0.889 |

: Intra-pool agreement statistics. {#tbl-agreement}

High agreement (>0.89) across all pools indicates that specialized agents within each domain reach consistent assessments.

### Consensus Convergence

We measure how consensus stabilizes as agents report:

| Agents Reported | Mean Error vs Final | Std Error |
|-----------------|-------------------|-----------|
| 32 (25%) | 0.047 | 0.023 |
| 64 (50%) | 0.021 | 0.012 |
| 96 (75%) | 0.008 | 0.005 |
| 115 (90%) | 0.003 | 0.002 |
| 128 (100%) | 0.000 | 0.000 |

: Consensus convergence. {#tbl-convergence}

After 90% of agents report, consensus differs from final by only 0.003---enabling early termination without accuracy loss.

## Fault Tolerance

### Crash Failure Tolerance

We simulate crash failures by randomly dropping agents:

| Crashes | Remaining | Accuracy | Degradation |
|---------|-----------|----------|-------------|
| 0 | 128 | 0.897 | 0.0% |
| 10 | 118 | 0.894 | 0.3% |
| 20 | 108 | 0.889 | 0.9% |
| 30 | 98 | 0.881 | 1.8% |
| 40 | 88 | 0.867 | 3.3% |

: Crash failure tolerance. {#tbl-crash-tolerance}

MACP tolerates **30 crash failures** (23%) with <2% accuracy degradation.

### Byzantine Failure Tolerance

We simulate Byzantine failures where agents report adversarial scores:

| Byzantine | Strategy | Accuracy | Error Bound (Theory) |
|-----------|----------|----------|---------------------|
| 0 | — | 0.897 | — |
| 5 | Random | 0.892 | 0.041 |
| 10 | Opposite | 0.878 | 0.089 |
| 15 | Coordinated | 0.856 | 0.134 |
| 20 | Worst-case | 0.823 | 0.178 |

: Byzantine failure tolerance. {#tbl-byzantine}

MACP maintains >85% accuracy with **13 Byzantine agents** (10%), matching the theoretical bound from Theorem 3.

## Scalability

### Agent Scaling

| Agents | Throughput (addr/sec) | Accuracy | Efficiency |
|--------|----------------------|----------|------------|
| 16 | 2,134 | 0.812 | 100% |
| 32 | 4,287 | 0.845 | 100% |
| 64 | 8,156 | 0.867 | 95.6% |
| 128 | 15,847 | 0.897 | 92.9% |
| 256 | 29,234 | 0.901 | 85.8% |

: Agent scaling results. {#tbl-scaling}

MACP achieves:

- **Near-linear throughput scaling** up to 256 agents
- **Diminishing accuracy returns** beyond 128 agents (0.4 pp improvement from 128→256)
- **93% scaling efficiency** at 128 agents

### Dataset Scaling

| Addresses | Time (sec) | Rate (addr/sec) |
|-----------|------------|-----------------|
| 10,000 | 0.63 | 15,873 |
| 100,000 | 6.31 | 15,847 |
| 546,247 | 34.47 | 15,847 |

: Dataset scaling (128 agents). {#tbl-dataset-scaling}

Processing time scales linearly with dataset size at constant throughput.

## Ablation Studies

### Pool Ablations

| Configuration | Accuracy | Δ |
|--------------|----------|---|
| Full MACP (4 pools) | 0.897 | — |
| − Wildfire Pool | 0.834 | −6.3 pp |
| − Flood Pool | 0.889 | −0.8 pp |
| − Seismic Pool | 0.892 | −0.5 pp |
| − Analytics Pool | 0.867 | −3.0 pp |

: Pool ablation. {#tbl-pool-ablation}

Wildfire and Analytics pools contribute most significantly.

### Consensus Mechanism Ablations

| Consensus Type | Accuracy | Notes |
|---------------|----------|-------|
| Confidence-weighted (MACP) | 0.897 | Full protocol |
| Uniform-weighted | 0.878 | All agents equal weight |
| Best single agent | 0.834 | No consensus |
| Median | 0.867 | Median score |
| Trimmed mean (10%) | 0.889 | Exclude extreme 10% |

: Consensus mechanism ablation. {#tbl-consensus-ablation}

Confidence-weighted consensus outperforms alternatives by 1-6 pp.

### Agent Count Ablations

| Agents per Pool | Total | Accuracy |
|-----------------|-------|----------|
| 4 | 16 | 0.812 |
| 8 | 32 | 0.845 |
| 16 | 64 | 0.867 |
| 32 | 128 | 0.897 |
| 64 | 256 | 0.901 |

: Agent count ablation. {#tbl-agent-ablation}

32 agents per pool (128 total) provides optimal accuracy/efficiency tradeoff.

## Real-World Validation

We validate against actual fire events:

| Fire Event | Year | Addresses | MACP Recall | Traditional GIS Recall |
|------------|------|-----------|-------------|----------------------|
| Tubbs Fire | 2017 | 21,207 | 92.3% | 78.4% |
| Camp Fire | 2018 | 18,934 | 89.7% | 71.2% |
| Dixie Fire | 2021 | 12,456 | 87.4% | 69.8% |
| Glass Fire | 2020 | 8,234 | 91.2% | 74.5% |

: Validation against historical fires. {#tbl-historical-validation}

MACP achieves **15-18 percentage point improvement** in recall over traditional GIS methods for high-risk addresses.

