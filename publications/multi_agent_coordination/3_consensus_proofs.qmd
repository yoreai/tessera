# Consensus Proofs

This section establishes the main theoretical results: optimality of weighted consensus, probabilistic accuracy bounds, and Byzantine fault tolerance.

## Optimality of Weighted Consensus

The central result is that our confidence-weighted consensus is statistically optimal.

::: {.callout-tip}
## Theorem 1 (Consensus Optimality)

Under Assumptions 1-2 (unbiased agents, confidence proportional to inverse variance), the weighted consensus:

$$
S^* = \frac{\sum_{i=1}^{n} c_i \cdot s_i}{\sum_{i=1}^{n} c_i}
$$

is the **Best Linear Unbiased Estimator (BLUE)** of the true risk $s_{\text{true}}$.
:::

**Proof.** We apply the Gauss-Markov theorem [@gauss1809theoria].

*Step 1: Define the estimation problem.*

We seek to estimate $s_{\text{true}}$ from observations $\{s_1, \ldots, s_n\}$ where:

- $\mathbb{E}[s_i] = s_{\text{true}}$ for all $i$ (unbiasedness)
- $\text{Var}(s_i) = \sigma_i^2$ where $\sigma_i^2 \propto 1/c_i$ (confidence-variance relationship)
- $\text{Cov}(s_i, s_j) = 0$ for $i \neq j$ (independence)

*Step 2: Characterize linear unbiased estimators.*

The class of linear estimators is:

$$
\mathcal{S} = \left\{ \hat{s} = \sum_{i=1}^{n} w_i s_i : w_i \in \mathbb{R} \right\}
$$

For $\hat{s}$ to be unbiased, we require:

$$
\mathbb{E}[\hat{s}] = \sum_{i=1}^{n} w_i \mathbb{E}[s_i] = s_{\text{true}} \sum_{i=1}^{n} w_i = s_{\text{true}}
$$

Thus $\sum_{i=1}^{n} w_i = 1$ is necessary and sufficient for unbiasedness.

*Step 3: Minimize variance.*

For unbiased estimators, the variance is:

$$
\text{Var}(\hat{s}) = \sum_{i=1}^{n} w_i^2 \sigma_i^2
$$

We minimize this subject to $\sum_i w_i = 1$ using Lagrange multipliers:

$$
\mathcal{L}(w, \lambda) = \sum_{i=1}^{n} w_i^2 \sigma_i^2 - \lambda\left(\sum_{i=1}^{n} w_i - 1\right)
$$

First-order conditions:

$$
\frac{\partial \mathcal{L}}{\partial w_i} = 2w_i \sigma_i^2 - \lambda = 0 \implies w_i = \frac{\lambda}{2\sigma_i^2}
$$

Substituting into the constraint:

$$
\sum_{i=1}^{n} \frac{\lambda}{2\sigma_i^2} = 1 \implies \lambda = \frac{2}{\sum_{j=1}^{n} \sigma_j^{-2}}
$$

Thus:

$$
w_i^* = \frac{\sigma_i^{-2}}{\sum_{j=1}^{n} \sigma_j^{-2}}
$$

*Step 4: Connect to confidence weights.*

By Assumption 2, $c_i \propto \sigma_i^{-2}$. Setting $c_i = \kappa \sigma_i^{-2}$ for some constant $\kappa > 0$:

$$
w_i^* = \frac{c_i}{\sum_{j=1}^{n} c_j}
$$

This matches our confidence-weighted consensus exactly. $\square$

::: {.callout-tip}
## Corollary 1 (Variance of Optimal Consensus)

The variance of the BLUE is:

$$
\text{Var}(S^*) = \frac{1}{\sum_{i=1}^{n} \sigma_i^{-2}} = \frac{1}{\sum_{i=1}^{n} c_i / \kappa}
$$

For homogeneous agents with $\sigma_i = \sigma$ and $c_i = c$:

$$
\text{Var}(S^*) = \frac{\sigma^2}{n}
$$
:::

This shows that variance decreases as $1/n$---the standard square-root improvement from averaging independent observations.

## Probabilistic Accuracy Bounds

We establish concentration bounds on the consensus estimate.

::: {.callout-tip}
## Theorem 2 (Chebyshev Bound)

For any $\varepsilon > 0$:

$$
\Pr\left(|S^* - s_{\text{true}}| \geq \varepsilon\right) \leq \frac{\text{Var}(S^*)}{\varepsilon^2}
$$
:::

**Proof.** Direct application of Chebyshev's inequality [@chebyshev1867inequality] to the unbiased estimator $S^*$. $\square$

::: {.callout-tip}
## Corollary 2 (Accuracy with Probability)

For homogeneous agents with common variance $\sigma^2$ and $n$ agents:

$$
\Pr\left(|S^* - s_{\text{true}}| \geq \varepsilon\right) \leq \frac{\sigma^2}{n \varepsilon^2}
$$

To achieve accuracy $|S^* - s_{\text{true}}| < \varepsilon$ with probability at least $1 - \delta$:

$$
n \geq \frac{\sigma^2}{\varepsilon^2 \delta}
$$
:::

**Example.** For $\sigma = 0.1$, $\varepsilon = 0.05$, and $\delta = 0.05$ (95% confidence):

$$
n \geq \frac{0.01}{0.0025 \times 0.05} = 80 \text{ agents}
$$

Our 128-agent system exceeds this requirement, achieving the accuracy bound with 99% probability.

## Byzantine Fault Tolerance

We now analyze robustness to Byzantine failures.

::: {.callout-note}
## Definition 10 (Byzantine Agents)

Let $B \subset \{1, \ldots, n\}$ denote the set of Byzantine agents with $|B| = k$. Byzantine agents may report arbitrary scores $s_i \in [0, 1]$ and confidences $c_i \in [0, 1]$.
:::

::: {.callout-tip}
## Theorem 3 (Byzantine Fault Tolerance)

With $k < n/3$ Byzantine agents, the consensus error is bounded:

$$
|S^*_{\text{faulty}} - s_{\text{true}}| \leq \frac{k}{n - 2k} \cdot \max_{i \in B} |s_i - s_{\text{true}}| + \frac{\sigma}{\sqrt{n - k}}
$$

where $S^*_{\text{faulty}}$ is the consensus computed with Byzantine agents included.
:::

**Proof.**

Let $H = \{1, \ldots, n\} \setminus B$ be the set of honest agents with $|H| = n - k$.

*Step 1: Decompose the consensus.*

$$
S^*_{\text{faulty}} = \frac{\sum_{i \in H} c_i s_i + \sum_{j \in B} c_j s_j}{\sum_{i \in H} c_i + \sum_{j \in B} c_j}
$$

Let $W_H = \sum_{i \in H} c_i$ and $W_B = \sum_{j \in B} c_j$ be the total weights of honest and Byzantine agents.

*Step 2: Bound Byzantine influence.*

In the worst case, Byzantine agents maximize their influence by:

- Reporting maximum confidence $c_j = 1$ for all $j \in B$
- Reporting extreme scores $s_j \in \{0, 1\}$ opposite to $s_{\text{true}}$

Thus $W_B \leq k$ and each Byzantine agent contributes error at most 1.

*Step 3: Bound honest consensus.*

The honest consensus $S_H = \sum_{i \in H} c_i s_i / W_H$ satisfies:

$$
\mathbb{E}[S_H] = s_{\text{true}}, \quad \text{Var}(S_H) \leq \frac{\sigma^2}{n - k}
$$

*Step 4: Combine bounds.*

The faulty consensus is:

$$
S^*_{\text{faulty}} = \frac{W_H}{W_H + W_B} S_H + \frac{W_B}{W_H + W_B} S_B
$$

where $S_B$ is the Byzantine contribution. The error is bounded by:

$$
|S^*_{\text{faulty}} - s_{\text{true}}| \leq \frac{W_B}{W_H + W_B} \cdot |S_B - s_{\text{true}}| + \frac{W_H}{W_H + W_B} \cdot |S_H - s_{\text{true}}|
$$

With $W_H \geq n - k$ (honest agents have confidence at least 1 on average) and $W_B \leq k$:

$$
\frac{W_B}{W_H + W_B} \leq \frac{k}{n - k + k} = \frac{k}{n}
$$

For the bound to be non-trivial, we need $k < n/3$, giving:

$$
|S^*_{\text{faulty}} - s_{\text{true}}| \leq \frac{k}{n - 2k} \cdot 1 + \frac{\sigma}{\sqrt{n - k}}
$$

$\square$

::: {.callout-tip}
## Corollary 3 (Maximum Tolerable Failures)

The maximum number of Byzantine failures that MACP can tolerate while maintaining consensus error below threshold $\tau$ is:

$$
k_{\max} = \left\lfloor \frac{n(\tau - \sigma/\sqrt{n})}{1 + 2\tau} \right\rfloor
$$

For $n = 128$, $\sigma = 0.1$, and $\tau = 0.15$:

$$
k_{\max} = \left\lfloor \frac{128(0.15 - 0.0088)}{1.30} \right\rfloor = \left\lfloor 13.9 \right\rfloor = 13
$$
:::

Thus our 128-agent system tolerates up to **13 Byzantine agents** (10% of total) while maintaining consensus error below 15%.

## Convergence Rate

We analyze how quickly the consensus stabilizes as more agents report.

::: {.callout-tip}
## Theorem 4 (Convergence Rate)

Let $S^*_m$ denote the consensus after receiving $m$ agent reports. Under random reporting order:

$$
\mathbb{E}\left[|S^*_m - S^*_n|\right] \leq \sqrt{\frac{n - m}{m(n - 1)}} \cdot \sigma
$$
:::

**Proof.** The consensus after $m$ reports is an estimate based on a random subset. By the properties of sample means:

$$
\text{Var}(S^*_m - S^*_n) = \sigma^2 \left(\frac{1}{m} - \frac{1}{n}\right) = \sigma^2 \frac{n - m}{mn}
$$

Applying Jensen's inequality:

$$
\mathbb{E}[|S^*_m - S^*_n|] \leq \sqrt{\text{Var}(S^*_m - S^*_n)} = \sigma \sqrt{\frac{n - m}{mn}}
$$

$\square$

**Practical Implication.** After receiving 90% of reports ($m = 0.9n$):

$$
\mathbb{E}[|S^*_m - S^*_n|] \leq \sigma \sqrt{\frac{0.1}{0.9 \cdot (n-1)}} \approx \frac{0.33\sigma}{\sqrt{n}}
$$

For $n = 128$ and $\sigma = 0.1$: expected change is $\leq 0.003$, negligible for risk assessment purposes. This enables **early termination**: we can emit preliminary results after 90% of agents respond without waiting for stragglers.

