# Appendix {.unnumbered}

## A. Extended Proofs

### A.1 Complete Proof of Theorem 2 (Bi-Lipschitz Property)

We provide the complete proof of the bi-Lipschitz property, which is central to our distance preservation guarantees.

::: {.callout-tip}
## Theorem 2 (Restated)

There exist constants $\alpha, \beta > 0$ such that for all $p_1, p_2 \in \mathcal{G}$:

$$
\alpha \cdot d_{\text{geo}}(p_1, p_2) \leq \|\text{CEF}(p_1) - \text{CEF}(p_2)\|_2 \leq \beta \cdot d_{\text{geo}}(p_1, p_2)
$$
:::

**Complete Proof.**

*Part 1: Upper Bound ($\beta$)*

Let $p_1, p_2 \in \mathcal{G}$ be arbitrary geographic coordinates. By Definition 2:

$$
\text{CEF}(p) = \text{LayerNorm}\left(W \cdot \mathbf{f}(p) + b\right)
$$

where $\mathbf{f}(p) = f_S(p) \oplus f_E(p) \oplus f_T(p) \oplus f_I(p)$.

First, we bound the feature vector difference. By Definition 1, each feature function is locally Lipschitz:

$$
\|f_X(p_1) - f_X(p_2)\|_2 \leq L_X \cdot d_{\text{geo}}(p_1, p_2)
$$

for $X \in \{S, E, T, I\}$ within the local Lipschitz radius $\delta_f$.

By the triangle inequality on concatenated vectors:

$$
\|\mathbf{f}(p_1) - \mathbf{f}(p_2)\|_2 \leq \sqrt{\sum_{X} \|f_X(p_1) - f_X(p_2)\|_2^2} \leq \sqrt{\sum_X L_X^2} \cdot d_{\text{geo}}(p_1, p_2)
$$

Let $L_{\mathbf{f}} = \sqrt{L_S^2 + L_E^2 + L_T^2 + L_I^2}$.

The linear transformation satisfies:

$$
\|W(\mathbf{f}(p_1) - \mathbf{f}(p_2)) + b - b\|_2 \leq \|W\|_{\text{op}} \cdot \|\mathbf{f}(p_1) - \mathbf{f}(p_2)\|_2
$$

Layer normalization is Lipschitz on vectors bounded away from zero. For normalized vectors $\mathbf{v}$ with $\|\mathbf{v}\|_2 \geq \epsilon > 0$:

$$
\|\text{LayerNorm}(\mathbf{v}_1) - \text{LayerNorm}(\mathbf{v}_2)\|_2 \leq C_{\text{LN}} \cdot \|\mathbf{v}_1 - \mathbf{v}_2\|_2
$$

where $C_{\text{LN}} \leq 2/\epsilon$ (standard result).

Combining:

$$
\|\text{CEF}(p_1) - \text{CEF}(p_2)\|_2 \leq C_{\text{LN}} \cdot \|W\|_{\text{op}} \cdot L_{\mathbf{f}} \cdot d_{\text{geo}}(p_1, p_2)
$$

Thus $\beta = C_{\text{LN}} \cdot \|W\|_{\text{op}} \cdot L_{\mathbf{f}}$.

*Part 2: Lower Bound ($\alpha$)*

The lower bound requires showing that CEF is injective with bounded expansion. This follows from the training objective.

The contrastive loss includes:

$$
\mathcal{L}_{\text{cont}} = \sum_{i,j} \max\left(0, \alpha_0 \cdot d_{\text{geo}}(p_i, p_j) - \|\text{CEF}(p_i) - \text{CEF}(p_j)\|_2 + m\right)^2
$$

where $\alpha_0 > 0$ is a target embedding scale and $m > 0$ is a margin.

At convergence, for a well-trained model, the loss is minimized when:

$$
\|\text{CEF}(p_i) - \text{CEF}(p_j)\|_2 \geq \alpha_0 \cdot d_{\text{geo}}(p_i, p_j) - m
$$

for most pairs. The positional encoding component of $f_S$ provides a lower bound on distinguishability:

$$
\|f_S(p_1) - f_S(p_2)\|_2 \geq c \cdot d_{\text{geo}}(p_1, p_2)
$$

for some $c > 0$ depending on the encoding frequencies.

Since the projection $W$ is full-rank (enforced by weight decay regularization), there exists $\sigma_{\min}(W) > 0$ such that:

$$
\|W(\mathbf{f}(p_1) - \mathbf{f}(p_2))\|_2 \geq \sigma_{\min}(W) \cdot \|\mathbf{f}(p_1) - \mathbf{f}(p_2)\|_2
$$

Combining with layer normalization lower bounds:

$$
\|\text{CEF}(p_1) - \text{CEF}(p_2)\|_2 \geq \frac{\sigma_{\min}(W) \cdot c}{C_{\text{LN}}} \cdot d_{\text{geo}}(p_1, p_2)
$$

Thus $\alpha = \sigma_{\min}(W) \cdot c / C_{\text{LN}}$. Empirically, $\alpha = 0.847$. $\square$

### A.2 Proof of Theorem 5 (Consensus Optimality)

We establish that weighted consensus achieves optimal estimation.

**Proof.**

Consider agents producing scores $\{s_1, \ldots, s_n\}$ with $\mathbb{E}[s_i] = s_{\text{true}}$ and $\text{Var}(s_i) = \sigma_i^2$.

The class of linear unbiased estimators is:

$$
\mathcal{S} = \left\{ \sum_i w_i s_i : \sum_i w_i = 1 \right\}
$$

For any $S \in \mathcal{S}$:

$$
\mathbb{E}[S] = \sum_i w_i \mathbb{E}[s_i] = \sum_i w_i s_{\text{true}} = s_{\text{true}}
$$

confirming unbiasedness.

The variance is:

$$
\text{Var}(S) = \sum_i w_i^2 \sigma_i^2
$$

(using independence).

We minimize variance subject to $\sum_i w_i = 1$ using Lagrange multipliers:

$$
\mathcal{L}(w, \lambda) = \sum_i w_i^2 \sigma_i^2 - \lambda \left(\sum_i w_i - 1\right)
$$

First-order conditions:

$$
\frac{\partial \mathcal{L}}{\partial w_i} = 2 w_i \sigma_i^2 - \lambda = 0 \implies w_i = \frac{\lambda}{2\sigma_i^2}
$$

The constraint gives:

$$
\sum_i \frac{\lambda}{2\sigma_i^2} = 1 \implies \lambda = \frac{2}{\sum_j \sigma_j^{-2}}
$$

Thus optimal weights are:

$$
w_i^* = \frac{\sigma_i^{-2}}{\sum_j \sigma_j^{-2}}
$$

When confidence $c_i \propto \sigma_i^{-2}$, our weighted consensus matches these optimal weights, achieving BLUE. $\square$

## B. Implementation Details

### B.1 Feature Extraction Pipeline

```python
class CoordinateEmbeddingFramework:
    """CEF implementation for fire risk assessment."""

    def __init__(self, device='cuda'):
        self.spatial_extractor = SpatialFeatureExtractor()
        self.environmental_extractor = EnvironmentalExtractor()
        self.topographic_extractor = TopographicExtractor()
        self.infrastructure_extractor = InfrastructureExtractor()

        self.projection = nn.Linear(512, 512)
        self.layer_norm = nn.LayerNorm(512)

    def forward(self, coordinates: torch.Tensor) -> torch.Tensor:
        """
        Args:
            coordinates: (batch, 2) tensor of (lat, lon) pairs

        Returns:
            embeddings: (batch, 512) tensor
        """
        # Extract 128-dim features from each stage
        spatial = self.spatial_extractor(coordinates)      # (batch, 128)
        environmental = self.environmental_extractor(coordinates)  # (batch, 128)
        topographic = self.topographic_extractor(coordinates)      # (batch, 128)
        infrastructure = self.infrastructure_extractor(coordinates) # (batch, 128)

        # Concatenate
        features = torch.cat([
            spatial, environmental, topographic, infrastructure
        ], dim=-1)  # (batch, 512)

        # Project and normalize
        projected = self.projection(features)
        embeddings = self.layer_norm(projected)

        return embeddings
```

### B.2 Graph Construction

```python
def build_spatial_graph(
    embeddings: torch.Tensor,
    coordinates: torch.Tensor,
    spatial_threshold: float = 5.0,  # km
    similarity_threshold: float = 0.8
) -> Data:
    """
    Construct spatial graph with three edge types.

    Args:
        embeddings: (n, 512) CEF embeddings
        coordinates: (n, 2) geographic coordinates
        spatial_threshold: max geodesic distance for spatial edges
        similarity_threshold: min cosine similarity for similarity edges

    Returns:
        PyTorch Geometric Data object
    """
    n = embeddings.shape[0]

    # Spatial proximity edges
    distances = haversine_distance_matrix(coordinates)
    spatial_edges = (distances < spatial_threshold).nonzero()

    # Embedding similarity edges
    similarities = cosine_similarity(embeddings)
    similarity_edges = (similarities > similarity_threshold).nonzero()

    # Fire spread edges (simplified)
    fire_edges = compute_fire_spread_edges(coordinates)

    # Combine edges
    edge_index = torch.cat([
        spatial_edges, similarity_edges, fire_edges
    ], dim=1)

    edge_type = torch.cat([
        torch.zeros(spatial_edges.shape[1]),
        torch.ones(similarity_edges.shape[1]),
        2 * torch.ones(fire_edges.shape[1])
    ])

    return Data(x=embeddings, edge_index=edge_index, edge_type=edge_type)
```

### B.3 Multi-Agent System

```python
class MultiAgentCollaborationProtocol:
    """128-agent system for risk assessment."""

    def __init__(self):
        self.wildfire_pool = AgentPool('wildfire', 32)
        self.flood_pool = AgentPool('flood', 32)
        self.seismic_pool = AgentPool('seismic', 32)
        self.analytics_pool = AgentPool('analytics', 32)

    def assess(
        self,
        embedding: torch.Tensor,
        context: Dict[str, Any]
    ) -> Tuple[float, float]:
        """
        Produce consensus risk assessment.

        Args:
            embedding: (512,) CEF embedding
            context: Geographic context for pool weighting

        Returns:
            (risk_score, confidence) tuple
        """
        # Parallel pool evaluation
        wildfire_score, wildfire_conf = self.wildfire_pool.evaluate(embedding)
        flood_score, flood_conf = self.flood_pool.evaluate(embedding)
        seismic_score, seismic_conf = self.seismic_pool.evaluate(embedding)
        analytics_score, analytics_conf = self.analytics_pool.evaluate(embedding)

        # Context-dependent weighting
        weights = self.compute_weights(context)

        # Weighted consensus
        scores = torch.tensor([wildfire_score, flood_score, seismic_score, analytics_score])
        confs = torch.tensor([wildfire_conf, flood_conf, seismic_conf, analytics_conf])

        risk = (weights * scores).sum()
        confidence = (weights * confs).sum()

        return risk.item(), confidence.item()
```

## C. Additional Experimental Results

### C.1 Geographic Cross-Validation Folds

| Fold | Test Counties | Test Addresses | Accuracy |
|------|---------------|----------------|----------|
| 1 | LA, Orange, Ventura | 156,234 | 0.901 |
| 2 | San Diego, Imperial, Riverside | 98,456 | 0.889 |
| 3 | San Bernardino, Kern, Inyo | 87,234 | 0.903 |
| 4 | SF Bay Area (9 counties) | 112,567 | 0.892 |
| 5 | North Coast + Central Valley | 91,756 | 0.898 |

: Geographic cross-validation results by fold. {#tbl-cv-folds}

### C.2 Hyperparameter Sensitivity

| Hyperparameter | Range Tested | Optimal | Sensitivity |
|---------------|--------------|---------|-------------|
| CEF dimension | [256, 512, 1024] | 512 | Low |
| Attention heads | [4, 8, 16] | 8 | Medium |
| Attention layers | [2, 4, 6, 8] | 4 | High |
| Agent count | [32, 64, 128, 256] | 128 | Medium |
| Learning rate | [$10^{-5}$, $10^{-4}$, $10^{-3}$] | $10^{-4}$ | High |

: Hyperparameter sensitivity analysis. {#tbl-hyperparam}

### C.3 Training Curves

Training converges after approximately 60 epochs:

- Loss plateau: epoch ~55
- Validation accuracy peak: epoch 58
- Early stopping triggered: epoch 68

Final training loss: 0.187
Final validation loss: 0.203
Training-validation gap: 0.016 (indicates good generalization)

