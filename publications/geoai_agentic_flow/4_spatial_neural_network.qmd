# Spatial Neural Network

Once coordinates are embedded via CEF, the Spatial Neural Network (SNN) applies graph-based reasoning to capture complex spatial relationships. The SNN treats embedded addresses as nodes in a dynamic graph, using attention mechanisms to propagate information along spatial and semantic edges.

## Graph Construction

Given a set of embedded coordinates $\{e_1, \ldots, e_n\}$ where $e_i = \text{CEF}(p_i)$, we construct a dynamic graph $G = (V, E)$:

**Nodes:** $V = \{v_1, \ldots, v_n\}$ with node features $h_i^{(0)} = e_i$.

**Edges:** We define three types of edges connecting nodes:

1. **Spatial Proximity Edges** ($E_{\text{spatial}}$): Connect nodes whose geographic coordinates are within distance $\delta_{\text{spatial}} = 5$ km:
$$
(v_i, v_j) \in E_{\text{spatial}} \iff d_{\text{geo}}(p_i, p_j) \leq \delta_{\text{spatial}}
$$

2. **Embedding Similarity Edges** ($E_{\text{sim}}$): Connect nodes with embedding similarity above threshold:
$$
(v_i, v_j) \in E_{\text{sim}} \iff \frac{e_i^T e_j}{\|e_i\|_2 \|e_j\|_2} \geq \tau_{\text{sim}} = 0.8
$$

3. **Fire Spread Edges** ($E_{\text{fire}}$): Connect nodes along potential fire propagation pathways (downwind, upslope):
$$
(v_i, v_j) \in E_{\text{fire}} \iff \text{fire\_reachable}(p_i, p_j) = \text{True}
$$

The combined edge set is $E = E_{\text{spatial}} \cup E_{\text{sim}} \cup E_{\text{fire}}$.

## Multi-Head Graph Attention

The SNN applies $L=4$ layers of multi-head graph attention [@wu2021gnn]:

$$
h_i^{(\ell+1)} = \text{LayerNorm}\left(h_i^{(\ell)} + \sum_{k=1}^{K} W_O^k \sum_{j \in \mathcal{N}(i)} \alpha_{ij}^k W_V^k h_j^{(\ell)}\right)
$$

where $K=8$ attention heads, $\mathcal{N}(i)$ denotes neighbors of node $i$, and attention weights are:

$$
\alpha_{ij}^k = \frac{\exp\left((W_Q^k h_i)^T (W_K^k h_j) / \sqrt{d_k}\right)}{\sum_{j' \in \mathcal{N}(i)} \exp\left((W_Q^k h_i)^T (W_K^k h_{j'}) / \sqrt{d_k}\right)}
$$

Here $d_k = 64$ is the head dimension, and $W_Q^k, W_K^k, W_V^k \in \mathbb{R}^{64 \times 512}$ and $W_O^k \in \mathbb{R}^{512 \times 64}$ are learned projections.

## Edge-Type-Specific Attention

Different edge types carry different semantic meanings. We parameterize attention by edge type:

$$
\alpha_{ij}^{k,t} = \frac{\exp\left((W_Q^{k,t} h_i)^T (W_K^{k,t} h_j) / \sqrt{d_k}\right)}{\sum_{j' \in \mathcal{N}(i)} \exp\left((W_Q^{k,t} h_i)^T (W_K^{k,t} h_{j'}) / \sqrt{d_k}\right)}
$$

where $t \in \{\text{spatial}, \text{sim}, \text{fire}\}$ indexes edge type. This allows the network to learn distinct attention patterns: spatial edges for local neighborhood context, similarity edges for semantic grouping, and fire edges for risk propagation.

## Position-Aware Attention

To preserve spatial information through the attention layers, we incorporate relative position encoding:

$$
\text{RelPos}(p_i, p_j) = [\Delta\phi_{ij}, \Delta\lambda_{ij}, d_{\text{geo}}(p_i, p_j), \angle(p_i, p_j)]
$$

where $\angle(p_i, p_j)$ is the bearing from $p_i$ to $p_j$. The relative position is projected and added to the attention logits:

$$
\text{logit}_{ij}^k = (W_Q^k h_i)^T (W_K^k h_j) + W_R^k \text{RelPos}(p_i, p_j)
$$

## Risk Score Aggregation

After $L=4$ attention layers, each node has an updated representation $h_i^{(L)}$ that incorporates neighborhood context. We produce per-node risk scores through a feedforward network:

$$
\text{risk}(v_i) = \sigma\left(\text{FFN}(h_i^{(L)})\right) \in [0, 1]
$$

where:
$$
\text{FFN}(x) = W_2 \cdot \text{GELU}(W_1 x + b_1) + b_2
$$

with $W_1 \in \mathbb{R}^{1024 \times 512}$, $W_2 \in \mathbb{R}^{1 \times 1024}$.

## Architecture Summary

```{python}
#| label: fig-snn-architecture
#| fig-cap: "Spatial Neural Network architecture showing the flow from embedded coordinates through graph attention layers to risk scores."

import sys
sys.path.insert(0, '..')
from _diagram_style import render_simple_flow_diagram

stages = [
    {'name': 'CEF Embeddings', 'description': '512-dim vectors', 'color': '#64b5f6'},
    {'name': 'Graph Build', 'description': '3 edge types', 'color': '#4db6ac'},
    {'name': 'Attention (×4)', 'description': '8 heads, typed', 'color': '#ffb74d'},
    {'name': 'Risk FFN', 'description': 'σ(MLP(·))', 'color': '#ef5350'},
]

render_simple_flow_diagram(stages, "Spatial Neural Network (SNN) Architecture", "snn_architecture.svg")
```

![SNN Architecture](snn_architecture.svg){#fig-snn-arch width=100%}

## Theoretical Properties

::: {.callout-tip}
## Proposition 1 (Expressiveness)

The SNN with $L$ layers can distinguish nodes whose $L$-hop neighborhoods differ structurally or in feature content.
:::

This follows from the Weisfeiler-Lehman characterization of graph neural network expressiveness. With edge-type-specific attention and position encoding, our SNN exceeds the expressiveness of standard message-passing networks.

::: {.callout-tip}
## Proposition 2 (Computational Complexity)

For graph with $n$ nodes and average degree $d$, the SNN has complexity:
$$
O(L \cdot K \cdot n \cdot d \cdot d_k^2 + L \cdot n \cdot d_{\text{model}}^2) = O(n \cdot d \cdot d_{\text{model}})
$$
:::

With $n \approx 10,000$ nodes per batch, $d \approx 50$ average neighbors, and $d_{\text{model}} = 512$, a single forward pass requires approximately 2.5 billion floating-point operations, completing in ~15ms on an A100 GPU.

## Training and Regularization

The SNN is trained end-to-end with the CEF using:

1. **Binary Cross-Entropy Loss** for risk classification:
$$
\mathcal{L}_{\text{BCE}} = -\sum_i \left[y_i \log(\text{risk}(v_i)) + (1-y_i) \log(1-\text{risk}(v_i))\right]
$$

2. **Attention Entropy Regularizer** to encourage diverse attention patterns:
$$
\mathcal{L}_{\text{ent}} = -\sum_{i,k} \sum_{j \in \mathcal{N}(i)} \alpha_{ij}^k \log(\alpha_{ij}^k)
$$

3. **Dropout** (rate 0.1) on attention weights and feedforward layers.

The combined loss is $\mathcal{L}_{\text{SNN}} = \mathcal{L}_{\text{BCE}} + 0.01 \cdot \mathcal{L}_{\text{ent}}$.

## Implementation Details

| Hyperparameter | Value |
|---------------|-------|
| Embedding dimension | 512 |
| Attention heads | 8 |
| Head dimension | 64 |
| Number of layers | 4 |
| FFN hidden dimension | 1024 |
| Dropout rate | 0.1 |
| Batch size | 8,192 nodes |
| Learning rate | $10^{-4}$ |
| Optimizer | AdamW ($\beta_1=0.9$, $\beta_2=0.999$) |

: SNN hyperparameters. {#tbl-snn-hyperparams}

The SNN processes the entire California address dataset (546,000 addresses) in approximately **27 seconds** when batched appropriately, enabling near-real-time risk assessment updates.

