import { NextRequest, NextResponse } from 'next/server'
import { 
  sampleMedicalTranscriptions, 
  samplePubMedAbstracts,
  sampleDrugReviews 
} from '@/lib/demo-data'

// Demo mode for Vercel deployment
const DEMO_MODE = process.env.ARESADB_DEMO !== 'false'

export async function POST(request: NextRequest) {
  try {
    const {
      query,
      table = 'medical_transcriptions',
      maxTokens = 4000,
    } = await request.json()

    if (!query || typeof query !== 'string') {
      return NextResponse.json(
        { error: 'Query is required' },
        { status: 400 }
      )
    }

    const startTime = performance.now()

    // Demo mode: Return sample RAG context
    if (DEMO_MODE) {
      await new Promise(resolve => setTimeout(resolve, 200 + Math.random() * 300))
      
      // Select demo data based on table
      let sourceData: any[] = []
      if (table.includes('transcription')) {
        sourceData = sampleMedicalTranscriptions.map(t => ({
          id: t.id,
          title: t.description,
          content: t.transcription,
          similarity: 0.85 + Math.random() * 0.1,
          metadata: { specialty: t.medical_specialty, keywords: t.keywords }
        }))
      } else if (table.includes('pubmed')) {
        sourceData = samplePubMedAbstracts.map(p => ({
          id: p.id,
          title: p.title,
          content: p.abstract,
          similarity: 0.80 + Math.random() * 0.15,
          metadata: { journal: p.journal, authors: p.authors }
        }))
      } else {
        sourceData = sampleDrugReviews.map(r => ({
          id: r.id,
          title: `${r.drug_name} - ${r.condition}`,
          content: r.review,
          similarity: 0.75 + Math.random() * 0.2,
          metadata: { rating: r.rating, drug: r.drug_name }
        }))
      }

      // Simulate context retrieval
      const chunks = sourceData.slice(0, 5).map(item => ({
        ...item,
        tokens: Math.floor(item.content.length / 4),
      }))

      const context = {
        chunks,
        totalTokens: chunks.reduce((sum, c) => sum + c.tokens, 0),
        query,
        table,
      }

      return NextResponse.json({
        success: true,
        context,
        retrievalTime: performance.now() - startTime,
        query,
        response: generateDemoResponse(query, context),
        demo: true,
      })
    }

    // Production mode: Use AresaDB CLI
    const { exec } = await import('child_process')
    const { promisify } = await import('util')
    const execAsync = promisify(exec)
    
    const ARESADB_PATH = process.env.ARESADB_PATH || '../../../tools/aresadb/target/release/aresadb'
    const DB_PATH = process.env.ARESADB_DB_PATH || '/tmp/aresadb-studio-demo'

    const contextCmd = `${ARESADB_PATH} --db ${DB_PATH} context "${query.replace(/"/g, '\\"')}" --table ${table} --max-tokens ${maxTokens} --format json`

    try {
      const { stdout: contextStdout } = await execAsync(contextCmd, {
        timeout: 30000,
        maxBuffer: 10 * 1024 * 1024,
      })

      let context
      try {
        context = JSON.parse(contextStdout)
      } catch {
        context = { chunks: [], totalTokens: 0 }
      }

      return NextResponse.json({
        success: true,
        context,
        retrievalTime: performance.now() - startTime,
        query,
        response: generateDemoResponse(query, context),
      })
    } catch (execError: any) {
      return NextResponse.json({
        success: false,
        error: execError.stderr || execError.message,
        retrievalTime: performance.now() - startTime,
      }, { status: 400 })
    }
  } catch (error: any) {
    console.error('RAG API error:', error)
    return NextResponse.json(
      { error: error.message || 'Internal server error' },
      { status: 500 }
    )
  }
}

function generateDemoResponse(query: string, context: any): string {
  // Demo response generator
  // In production, this would call OpenAI/Anthropic/etc.

  const chunks = context.chunks || []
  const numSources = chunks.length

  return `Based on ${numSources} relevant documents from the medical knowledge base:

This is a demonstration response for the query: "${query}"

In a production deployment, this would be generated by an LLM (OpenAI, Anthropic, etc.)
using the ${context.totalTokens || 0} tokens of retrieved context.

Key sources retrieved:
${chunks.slice(0, 3).map((c: any, i: number) =>
  `${i + 1}. ${c.title || 'Document'} (similarity: ${(c.similarity * 100).toFixed(1)}%)`
).join('\n')}

The RAG pipeline successfully retrieved relevant context for your query.`
}

